---
title: '论文记录-Best reply structure and equilibrium convergence in generic games'
date: 2021-11-11 10:19:36
tags: game theory
categories: 论文
description: 博弈论-矩阵震荡
---
# 通用博弈中的最优回复结构和均衡收敛

## 摘要
博弈论被广泛用于模拟相互作用的生物和社会系统。在某些情况下，博弈者可能会趋同于一个均衡，例如纳什均衡，但在另一些情况下，他们的战略动态会内生振荡。如果系统的设计不是为了鼓励趋同，那么我们可以先验地预期这两种行为中的哪一种呢？为了解决这个问题，我们采用了理论生态学中流行的一种方法来研究生态系统的稳定性： 我们根据可能代表现实世界博弈属性的约束条件随机生成回报矩阵。我们的研究表明，最优回复循环是博弈中的基本拓扑结构，它预示着六种著名的学习算法不会收敛，这些算法在生物学中得到了应用，或在人类玩家的实验中得到了支持。最优回复循环在复杂的竞争性博弈中占主导地位，这表明在这种情况下，均衡通常是一个不切实际的假设，我们必须明确地建立学习动态模型。
## Introduction
博弈论是一套针对决策者之间战略互动的数学模型，可应用于各种问题，如合作的出现、语言的形成以及道路和互联网的拥堵。同样的数学工具也被用于生态学和种群生物学中的进化建模。一个长期存在的问题是，当玩家通过反复玩游戏进行学习时，他们是否会趋同于一个均衡点。人们对一般情况知之甚少，因为答案取决于博弈的属性和学习算法。在这里，我们引入了一种我们称之为最优回复结构的形式主义，它可以粗略度量各种学习算法在任何双人正态博弈中的收敛概率。生态学中的定性稳定性理论和使用雷诺数来理解流体动力学中的湍流都可以类比说明我们的方法在其他领域的实用性。

解决博弈论均衡收敛问题的标准方法是关注具有特殊数学性质的博弈类，选择它们作为现实世界情景的程式化模型。例如，在[势博弈](https://www.zhihu.com/question/26666055)（potential games，也翻译成潜在博弈）中，单方面改变策略的所有收益差异都可以用一个全局势函数来表示;  [拥塞博弈](https://www.zhihu.com/question/481180778)属于这一类，所以势博弈可以作为交通的程式化模型。势博弈中大多数学习算法收敛到一个纳什或相关的均衡在潜在的对策，在优势可解博弈（dominance-solvable game）、协调博弈（coordination game）、超模博弈（supermodular game）和弱非循环博弈（weakly acyclic game）中也是如此。研究具有特殊属性的博弈在机制设计等某一方玩家可以选择博弈形式的情况下是非常有用的。在这些情况下，也可以设计玩家将要使用的学习算法，并选择最有可能收敛到均衡的算法，例如在线拍卖和网络路由。

然而，在其他一些问题中，博弈和学习算法并不是设计出来的，而是由设置的内在性质决定的。例如，金融市场可以被看作是一场博弈，其中有许多可能的行动与交易者可以买卖的资产相对应。结果可能趋于均衡，也可能内生波动。如果系统的设计不是为了鼓励趋同，我们应该期待这两种行为中的哪一种？

为了解决这个问题，我们采用了一种在理论生态学和发育生物学中卓有成效、在物理学中也很普遍的方法。梅（May）的理论生态学开创性论文就是这种方法的一个例子。他研究了一组随机产生的捕食者-猎物相互作用，并将其作为一般生态系统的空模型。他的主要结果是，随机生态系统往往会随着规模的扩大而变得更加不稳定。当然，梅很清楚，真实的生态系统并不是随机的；相反，它们是由进化选择和其他力量塑造而成的。许多大型生态系统已经存在了很长时间，这表明它们实际上是稳定的。因此，这一矛盾表明，真实的生态系统并不是梅模型中使用的随机集合的典型成员，这就提出了一个重要的问题：这些生态系统究竟是如何变得不典型的，它们又是如何以及为什么进化成稳定的？45 年后的今天，对这一问题的正确回答仍然是一个活跃的研究课题。例如，约翰逊等人最近发现，真实的生态系统具有一种他们称之为营养相干性的特性，并证明将这种特性作为随机生成的生态系统集合的约束条件可以确保稳定性。

在这里，我们将类似的方法应用于博弈论，将随机生成的双人博弈集合作为一个空模型。出于可操作性的考虑，我们利用可以系统地枚举所有可能博弈的事实，研究正态博弈。在这里，我们详细研究了一个参数$G$，它可以调整两位博弈者收益的相关性。这可以调节博弈中的竞争强度，并将零和博弈作为 $G = -1$ 的一个特例。我们还简要介绍了如何构建其他约束条件，例如，研究潜在博弈的偏差。通过这种方法，我们可以看到偏离特定类别博弈会如何影响学习动态的稳定性。

随机生成的博弈和一般学习算法不具备允许精确求解的数学特性。为了克服这一局限性，我们开发了一种形式主义，以简单指标的函数形式获得收敛的近似概率。通过与流体力学的类比，我们可以清楚地了解这些近似解是如何发挥作用的。当流体被驱离平衡状态时，通常会从稳定流（或层流）过渡到不稳定流（或湍流）。描述流体动力学的纳维-斯托克斯方程没有解析解，但这种过渡可以用一个称为雷诺数 (7) 的非一维参数来粗略描述。雷诺数越大，发生湍流的可能性就越大。尽管这一预测并不精确--它只是一条经验法则--但却非常有用。我们对博弈的类似估计没有简单的封闭形式，但却具有类似的预测能力。与我们的方法类似的另一种方法是理论生态学中的定性稳定性理论 (6)。生态学中的许多模型都会考虑食物网中不同物种之间相互作用的程度。例如，这些模型会考虑兔子吃了多少草，狐狸吃了多少兔子。而定性稳定性方法只考虑捕食者与被捕食者之间关系的符号--兔子吃草，狐狸吃兔子。这样，仅从食物网的拓扑特性就可以对生态系统的稳定性进行近似评估。与定性稳定性理论一样，我们的方法只依赖于博弈的拓扑特性，而不依赖于收益的细节。

我们的形式主义基于最优回复动力学，在此动力学下，每个玩家都会对对手的最后一个行动做出近视的最优回复。最优回复动力学在博弈论中是众所周知的，并被广泛用于发展学习的直觉，但我们以一种新的方式使用它，以获得一般博弈中收敛的近似概率。在最优回复动力学条件下，系统要么会渐进地收敛到一个与纯策略纳什均衡相对应的固定点，要么会陷入一个循环。我们根据最优回复循环与博弈固定点的相对大小，考虑了一个非常简单的博弈不收敛指标。请注意，我们并没有假设玩家遵循最优回复动态。相反，我们假设回报矩阵的最优回复结构构成了一阶骨架，形成了玩家试图学习的博弈的主干，这对于理解许多学习算法的收敛性非常有用。

为了验证这一假设，我们选择了一组从真人博弈实验中得到的学习算法，包括强化学习、[虚拟博弈](https://blog.csdn.net/weixin_40814740/article/details/109734014)、有噪声和无噪声的经验加权、k-level学习。我们也引入了（二种群）复制因子动力学在生态学和种群生物学中的重要性。基于最优回复动力学的方法预测了这些算法在$R^2≥0.78$时的不收敛频率。

在此，我们要强调的是，我们的目标是描述性的，而不是规范性的。在机制设计中，人们或机器使用的算法被设计为具有良好的收敛特性。例如，有些算法会收敛到相关均衡，这是纳什均衡的一种概括，允许博弈者在所有博弈中根据共同的信号进行协调。遗憾匹配就是这样一种算法，在这种算法中，玩家会考虑过去所有的博弈历史，并计算如果他们采取任何其他行动，他们的回报会是多少。虽然这些算法可以由机器或有足够记录能力的人来执行，但除非经过专门训练，否则不太可能被真人使用。据我们所知，这些算法只有间接的经验支持。我们将重点放在经过实验测试的算法上。当达到一个固定点时，这些算法会收敛到纳什均衡或接近纳什均衡的点，而不是更一般的相关均衡。

在证明最优回复结构形式主义可行之后，我们分析了最优回复周期或固定点是如何随着博弈的两个属性的变化而变化的。我们根据玩家可用的行动数$N$来定义一个博弈的复杂程度。简单的博弈只有几个行动，而复杂的博弈则有很多行动。博弈的竞争性由两名玩家的收益之间的相关性$G$来定义。相关性越负，博弈的竞争性越强。当我们改变博弈的这两个属性时，最优回复循环与固定点的相对比例会跟踪我们所考虑的六种算法的收敛频率，竞争性博弈中的虚构游戏除外。

我们的研究表明，在一端，简单且无竞争性的博弈不太可能有循环，而在另一端，复杂且有竞争性的博弈很可能有循环。我们之前提到的几类博弈，即潜在博弈、优势可解博弈、协调博弈、超模态博弈和弱非循环博弈，在构造上都是非循环的。这些类别中的任何一类都可能是简单非竞争博弈集合的典型成员，在这些博弈中，非循环行为很常见，但对于复杂竞争博弈来说，它们肯定不是典型的非循环行为。这些结果与我们的直觉相吻合，即复杂博弈更难学习，当一个博弈者的收益就是另一个博弈者的损失时，博弈者就更难协调均衡。我们的形式主义可以量化这一点。例如，在每个玩家有两个行动、两个玩家的收益之间没有相关性（$G=0$）的情况下，非循环博弈约占总数的 85%。然而，在$N=10$和$G=-0.7$的情况下，非循环博弈只占总数的 2.7%。

我们还展示了如何利用最优回复形式主义来研究给定博弈类别（如势博弈）的稳定性，以了解它们在偏离给定类别情况下的稳定性。我们展示了这种行为可能是非线性的，例如，势博弈的微小扰动会导致不收敛性不成比例地大幅增加。

在收益不相关（$G=0$）的情况下，我们使用受统计力学启发的组合方法来分析计算不同长度的最优回复周期的频率。使用受统计力学启发的方法并不是博弈论中的新想法。以前的研究已经量化了纯策略纳什均衡、混合策略均衡和帕累托均衡的属性，但我们是第一个量化最优回复周期的频率和长度并展示其与学习相关性的研究。加拉和法默以前用不同的方法研究了随机博弈中经验加权吸引力在$N\rightarrow \infty$时的收敛频率；在这里，我们将其扩展到任意$N$，研究了一系列不同的学习算法，并对不稳定性的起源有了更深入的了解。我们引入的形式主义可以向多个方向扩展，并应用于不同领域。例如，我们的研究结果也与通过复制器动力学的食物网稳定性有关，并可映射到布尔网络中。

当收敛到均衡状态失败时，我们经常会观察到混乱的学习动态。当这种情况发生时，博弈者并没有收敛到任何形式的时际“混沌均衡”，即他们的预期与博弈结果不匹配，甚至在统计意义上也是如此。在许多情况下，由此产生的吸引子维度很高，这使得“理性”的玩家很难通过统计方法预测其他玩家的行动，从而获得优于其他玩家的结果。一旦至少有一名玩家系统性地偏离均衡状态，学习和启发式方法就能超越均衡思维，并能更好地描述玩家的行为。

我们首先开发了最优回复框架。接下来，我们将展示如何利用它来预测我们在此研究的六种学习算法的非收敛频率，首先提出一些论点，给出一些直观的解释，说明为什么这样做可行，然后提供更多量化证据。然后，我们研究了最优回复循环是否会随着博弈的某些属性的变化而变得普遍，并说明了两个不同的约束条件的影响，这两个约束条件代表了与众所周知的博弈类别的偏差。最后，我们开发了一种分析性组合方法，用于计算在收益不相关的情况下最优回复循环的频率。
## Results
### Best reply structure
首先，我们将介绍一个框架，该框架将证明，我们所分析的学习算法系列将收敛到一个固定点的可能性可以得到一个有用的估计。正如我们将演示的，这提供了一种框架，可以通过分析给出算法在玩家尝试学习游戏时将遇到的稳定性问题的一阶近似值。表 1 总结了我们将要介绍的术语。

|Table 1. Terminology.|NE, Nash equlibrium.|
|---|---|
|Best reply|Action that gives the best payoff in response to a given action by an opponent|
|Best reply structure|Arrangement of the best replies in the payoff matrix|
|Best reply matrix|Derived payoff matrix, with one for the best reply to each possible move of the opponent and zero everywhere else|
|Best reply dynamics|Simple learning algorithm in which the players myopically choose the best reply to the last action of their opponent|
|Best reply k-cycle|Closed loop of best replies of length k (each player moves k times)|
|Best reply fixed point|Pure NE, i.e., the action for each player that is a best reply to the move of the other player|
|Best reply vector u|List of the number of distinct attractors of the best reply dynamics, ordered from longest cycles to fixed points|
|Free action/free best reply|Best reply to an action that is neither part of a cycle nor a fixed point|
|Best reply configuration|Unique set of best replies by both players to all actions of their opponent|
假设在一个双人正则表达式博弈中，玩家分别是行玩家和列玩家，各自的行动（或移动，或纯策略）记作$i, j = 1, ..., N$。最佳回应是指在回应对手给定行动时给出最佳回报的行动。最佳回应结构是最佳回应在收益双矩阵（即描述双方收益的两个矩阵）中的排列。(本文中，我们将使用 "收益矩阵 "一词来指双矩阵）在最佳回复动态下，每个玩家都会对对手的最后一个行动做出最佳回复。我们考虑的是最佳回应动态的一个特殊版本，即两位玩家交替下棋，各自选择对对手最后一个行动的最佳回应。

为了理解基本思想，我们考虑如下图所示的$N=4$的博弈。在图中，$S^R=\{1,2,3,4\}$和$S^C=\{1,2,3,4\}$分别是行玩家和列玩家的可能行动，矩阵中的每个格子反映了他们各自的收益，行玩家在左边。最优回复箭头指向了这个格子对应的最优回复格，垂直箭头对应行玩家，水平箭头对应列玩家。红色箭头表示该箭头是循环的一部分，橘黄色表示该箭头不在循环里，但会导向循环，蓝色表示直接指向固定点，天蓝色表示通过不止一步导向固定点。图B中的最优回复矩阵是布尔约简，其构造为具有与图A中的收益矩阵相同的最佳回复结构，但只有1和0作为其条目。假设我们选择了$(1,1)$作为初始条件，假设列玩家先行动，作为对$S^R=1$的最优回应，他选择了$S^C=2$。接下来行玩家行动，最优回应是$S^R=2$，由此列玩家又会再移动到$S^C=1$，等等。这样两个玩家将陷入循环$(1,1)\rightarrow(1,2)\rightarrow(2,2)\rightarrow(2,1)\rightarrow(1,1)$。我们将这样的循环称为最优回复双循环，因为每个玩家都移动两次。这个循环是一个吸引子，如果从$(3,2)$开始博弈，且行玩家先行，则博弈还是会进入到这个循环。先行玩家可以随机抽选，如果玩家的行动已经在循环了，那先行者的抽选对结果没有任何影响，因为至多有一个玩家有动机偏离当前的情况。然而如果当前不在循环，则玩家行动顺序就重要起来了。例如在这个例子中，存在两个吸引子，还是从$(3,2)$开始，列玩家先行，则博弈会一步到达最优回复固定点$(3,3)$，它是一个纯策略纳什均衡（NE）。
![图1](https://github.com/likun1208/image/blob/master/bestreply-1.png?raw=true)
给定$N\times N$的收益矩阵$\Pi$，我们通过最优回复向量$v(\Pi)=(n_N,...,n_2,n_1)$来表征一组最优回复动力学的吸引子，其中$n_1$是固定点的数量，$n_2$是双循环的数量，以此类推。例如，图1中的向量是$v=(0,0,1,1)$，表示有1个固定点和1个双循环。

如上图B所示，将收益矩阵还原为最优回复矩阵是非常有用的。具体做法是将每位玩家的所有最佳回复替换为 1，所有其他条目替换为 0。最佳回复矩阵的最佳回复结构与其衍生的收益矩阵相同，但它忽略了收益的其他方面。最佳回复矩阵具有与原始博弈相同的循环和纯策略近似值，但一般来说，混合策略近似值有所不同。一旦知道了最佳回复动态的吸引子，就可以列出所有的混合策略近似值。每个最佳回复循环都有一个混合 NE，循环和固定点的所有可能组合都有一个混合 NE。与给定循环相关的混合策略近似值对应于随机走每一步，其频率与循环中出现的频率相同。例如，图 1B 中与最佳回复周期相关的混合策略均衡为$x,y=(0.5, 0.5, 0, 0), (0.5, 0.5, 0, 0)$。与每种可能的循环和固定点组合相关的混合策略 NE 对应于组合行动集的平均值。例如，在图 1B 中，与循环和固定点组合相关的混合 NE 为 $x, y = (0.33, 0.33, 0.33, 0),(0.33, 0.33, 0.33, 0)$。对于最佳回复矩阵，不存在其他混合策略近似点。

在从最佳回复矩阵到原始博弈的过程中，一些混合近似解可能会存活下来，而另一些则可能不会，并且可能会引入新的混合近似解。例如，在图 1A 中，它们都没有存活下来，而且在 $x, y = (0.32, 0, 0, 0.68), (0.36, 0.64, 0, 0)$ 和 $x, y = (0.26, 0.15, 0, 0.59), (0.32, 0.24, 0.44, 0)$处有两个混合均衡，它们与相关的最佳回复动态没有关系。我们在第 S2 节中对随机生成的 1000 个 $N = 10$ 的博弈进行了量化说明。
### Learning dynamics
为了解决学习何时收敛的问题，我们研究了六种不同的学习算法。这些算法的选择跨越了不同的信息条件和理性水平。我们的重点是有经验支持的算法。这包括在生物学中用于动物学习或种群遗传学等目的的算法，以及用于在实验室环境中进行人类游戏行为实验的算法。我们在 "材料与方法 "中对六种算法逐一进行了简短总结，并在第 S1 节中介绍了详细内容。在此，我们仅讨论每种算法背后的基本思想，并说明将其纳入本研究的原因。

强化学习基于这样一种理念，即玩家更有可能采取过去收益更好的行动。它是标准的学习算法，用于信息有限和/或没有复杂推理的情况下，例如动物学习。我们研究的是布什-莫斯泰勒（Bush-Mosteller）算法，该算法已在实验中进行了广泛测试。

虚拟博弈需要更多的复杂性，因为它假定玩家构建了一个对手的心理模型。每个玩家都假定对手过去行动的经验分布就是她的混合策略，并根据这一信念下出最佳对策。一种常用的变体——加权虚拟博弈——假定过去的行动会打折扣。关于标准虚拟博弈和加权虚拟博弈的经验证据不一。从理论的角度来看，标准虚拟博弈在许多情况下会趋同于混合策略 NE，而加权虚拟博弈则不会。正如我们将看到的，如果学习算法经常达到混合均衡，我们的最佳回复形式主义就不能很好地发挥作用。我们选择标准版本的虚拟博弈来说明这一点，因为它提供了更有力的检验。

复制动力学常用于生态学和种群生物学。它表示种群中某些性状随世代的演变，每个性状的适合度取决于种群中所有其他性状的份额。复制动力学也可以看作是一种学习算法，其中每个性状都对应着一种行动。在这里，我们考虑的是双种群复制动力学，而不是更标准的单种群版本，因为在单种群版本中，两个参与者的收益矩阵顾名思义是对称的，而我们要研究的是收益的相关性。

经验加权吸引力（EWA）已被提出，用于归纳强化学习和虚拟博弈，并已被证明能很好地适应实验数据。其成功的关键在于一个权衡已实现报酬和已放弃报酬的参数。EWA 还包括其他参数，如记忆和报酬敏感性。与上述三种学习算法不同，其参数是决定收敛性的关键： 对于某些参数值，它总是收敛到固定点，而这些固定点可能离纳什均衡或相关均衡任意遥远。正如我们在 "补充材料 "中详细讨论的那样，由于缺乏对一般博弈中参数的实验指导，我们选择的参数值有可能使收敛接近近邻均衡。EWA 不会收敛到更一般的相关均衡，除非它们对应于NE。

上述算法都基于批量学习的概念，这很方便，因为这意味着它们都是确定性的。在批量学习中，棋手在更新自己的策略之前会大量观察对手的行动，因此会根据对手在每个时间点的实际混合策略进行学习。确定性假设有助于从数字上识别静止状态。在许多实验情况下，更现实的假设是，棋手在观察对手的单次行动后更新策略，而对手的行动是从其混合策略中随机取样的。这就是所谓的在线学习。作为在线学习的一个例子，我们研究了 EWA 的随机版本。

上述五种算法都是向后看的。为了与具有前瞻性行为的例子进行比较，我们使用了水平-k 学习或预期学习，其中棋手试图通过提前 k 步思考来战胜对手。level-k思维的观点得到了相当多的经验支持，一些研究还特别支持预期学习。在这里，我们通过使用第 2 层 EWA 学习来实现 k 级推理。两位棋手都假设对手是一级学习者，并使用 EWA 更新策略。因此，棋手会尝试根据对手的预测行动先发制人，而不是仅仅根据对手的历史行动频率采取行动。这为前瞻性行为提供了一个衡量标准。

总之，我们之所以选择这六种算法，是因为它们本身就很重要，而且能说明不同的特性。我们感兴趣的是生态学和生物学中使用的算法，或者在实验室人类实验中观察到的有界理性算法。我们特别不研究那些与计算机科学或机制设计相关，但与行为学观点无关的复杂算法。这些算法的收敛特性可能与本文研究的算法截然不同。遗憾匹配就是我们不研究的这类算法的一个例子，它被认为是一种 "自适应启发式"，是有界理性和近视的。然而，它仍然要求博弈者计算如果他们在之前的所有时间步骤中采取任何其他行动，他们会得到什么回报，而且没有经验证据表明人类实验中的博弈者会使用它。其他能达到相关均衡的算法，如校准学习法，则更为复杂。
### How does the best reply structure shape learning dynamics?

### Quantitative evidence

### Variation of the best reply structure

### Analytical approach

## Discussion

## Materials and methods

### Reinforcement learning

### Fictitious play

### Replicator dynamics

### Experience-weighted attraction

### EWA with noise

### Level-k learning

### Payoff matrices