---
title: 论文记录-Eliciting Thinking Hierarchy without a Prior
date: 2022-10-13 09:58:32
tags:
    - game theory
    - crowdsourcing
categories: 论文
description: 没有先验知识的情况下从众多答案中选择正确的那个
---

# Eliciting Thinking Hierarchy without a Prior

## Abstract

针对无法验证答案的众包任务，传统思路：选择大多数人支持的答案

存在问题：大多数人犯系统性错误，会导致最终结果出错

本文希望：在没有任何先验的情况下，在所有答案中建立一个等级制度，使得根据该制度得到的排名较高的答案（可能不被大多数人支持）来自更可靠的人

本文提出：

1. 一个新的模型来描述人们的思维层次

2. 两种算法来学习没有任何先验的思维层次

3. 一种基于上述理论框架的新的公开回答的众包方法

本文实验：

1. 本文方法所学习到的排名靠前的答案的准确度远远高于全体投票（在一个问题中，全体投票的答案得到了74名受访者的支持，但正确答案只得到了3名受访者的支持。我们的方法在没有任何先验的情况下将正确答案排在最高位置）

2. 本文模型有很高的拟合度，特别是对于我们排名第一的答案是正确的问题

## Introduction

群体智慧在面对系统性错误的时候往往会得出错误结果

一个例子：圆A的半径是圆B半径的1/3，圆A绕着圆B转了一圈后回到起点。圆A总共会旋转多少次？

![圆圈问题](https://github.com/likun1208/image/blob/master/2109.jpg?raw=true)

根据实验，有11个人认为是1次，8个人选2次，134个人选3次，16个人选4次，27个人选6次，21个人选9次。显然在多数一致的众包任务中，这个题的答案会被认为是3次，单其实正确答案是4次，只有16个人选。

如果事先了解所有受访者的先验水平，我们也许能根据大家的专业水平来确定正确答案，但是在大多数众包场景中，往往没这样的先验知识。

为了解决上述问题，Prelc等人[15]提出了一种创新的方法，他们准备了多个选择，要求受访者挑选一个选项，并预测其他人的选择分布。他们使用预测结果来构建一个关于选择的先验分布，然后选择比先验分布更受欢迎的选择，这样就纠正了偏见（是2017年的nature）。许多其他工作[10, 5, 16]发展了使用先验或预测来纠正偏差的想法。

然而，首先，将以前的方法应用于运行中的例子，即圆圈问题，是不适用的，因为它们需要先验知识来设计选择。让受访者报告所有选择的整体分布也很费力。第二，以前的工作主要是利用预测来纠正偏见，而利用他们的预测来建立一个思维层次是内在的有趣。这也导致了答案之间的层次性。著名的认知分层理论（CHT）[21, 22, 3]在人们玩游戏的场景中建立了一个思维理论，这样我们就可以学习不同复杂程度的玩家的行动。然而，CHT只是为游戏理论环境设计的。我们对在一般的问题解决场景中建立一个思维理论感到好奇。该问题的关键在于，水平较高的人知道水平较低的人的想法，但反之则不然[3, 9]。我们想应用这个见解来学习不同复杂程度的人的答案，称为思维层次，而不需要任何先验。

**关键问题**：我们的目标是建立一个实用的方法，在没有任何先验的情况下学习思维层次。基于思维层次，我们可以对答案进行排序，这样，排名较高的答案，可能不被大多数人支持，但是来自更可靠的人。

层次性思维方式的优势除了在于选出正确答案以外，还有以下几点：

1. 一些主观性问题可能有多个高质量答案，完整的层次结构能提供更丰富的结果。

2. 答案的层次性有助于更好地理解人们的思维方式，尤其当我们试图征求人们对某项政策的意见时这一点更重要。

**本文方法**：和以往研究一样，让受访者同时写出自己的问题和对其他人答案的预测，并将其扩展到一个更实用的基于开放回答的范式。该范式提出了一个单一的开放回答问题，并要求每个回答者的答案和对其他人的答案的预测。例如，在圆圈问题中，一个回答者可以提供：答案。"4"，预测。"3"。然后我们构建一个答案-预测矩阵A，记录报告特定答案-预测对的人数。（例如，图2（a）显示有28人回答 "3"，预测其他人回答 "6"）。

![圆圈问题的答案-预测矩阵](https://github.com/likun1208/image/blob/master/fig2.png?raw=true)

为了学习思维层次，本文提出了一个新颖的模型，它描述了不同复杂程度的人如何回答问题，和预测其他人的答案。回答者的答案和预测的联合分布取决于描述人们思维层次的潜在参数。本文表明，鉴于回答者的答案和预测的联合分布，可以通过解决非负矩阵分解问题的一个新变体来推断潜在的思维层次，该问题被称为非负全等三角化（NCT）。基于对NCT的分析，本文提供了两个简单的答案排序算法，并表明在适当的假设下，给定应答者的答案和预测的联合分布，这些算法将学习潜在的思维层次。

> 直接一些来说，就是专业人士不仅能答对，还能知道业余者会犯什么错。

最后，本文将答案和预测的联合分布表示为答案-预测矩阵，并在该矩阵上实现了基于NCT的答案排序算法。默认的排序算法是使矩阵上三角区域内的元素的平方和最大化，在变体的排序算法中，为了让不同的答案具有相同的复杂程度，答案被分割以压缩矩阵，该算法使压缩后的矩阵的上三角区域的平方和最大化。

在实验方面，进行了包括数学、围棋、常识和字符发音的实证研究。

需要说明的是：在前面那个矩阵图中，因为默认算法不需要用对角线的元素，所以这里对角线上的元素不再是答案-预测的人数，而是选择了这个答案的人数，这样方便和普通的多数一致众包方法做对比，例如那个矩阵里`(3,3)`对应的134表示有134个人回答了3，而不是说这么多人自己回答3也预测别人回答3。

### Related Work

先略

## Learning Thinking Hierarchy
### Thinking hierarchy
给定一个问题`q`（例如圆圈问题），`T`表示思维类型的集合，`A`表示可能出现的回答的集合，这两个都是有限集，$\Delta_A$表示`A`的所有可能分布。

**Generating answers**：说明不同思维类型的人是如何得出答案的。

**定义2.1**：思维类型`W`的预言机：一个生成预言机的答案（an anwser generating oracle，不太理解这是指生成预言机的答案还是指生成答案的预言机）将问题映射到集合`A`中的（随机）答案。每一个类型`t`都对应了一个预言机$O_t$，$O_t(q)$的输出是一个随机变量，其分布是$w_t\in \Delta_A$。$W$是一个$|T|\times|A|$矩阵，其中每一行是$w_t$。

每一个受访者都有概率$p_t$是类型$t$，且$\sum_t p_t=1$，类型`t`的受访者通过运行预言机$O_t$得到自己的答案。对于所有$a\in A$，一个受访者回答$a$的概率是$\sum_t p_tw_t(a)$，假设对于所有$a\in A$，概率都是正的。

**例2.2（运行示例）**：有两种思维类型$T=\{0,1\}$，回答空间是$A=\{3,4,6\}$，预言机$O_0$有0.8的概率输出3，0.2的概率输出6；$O_1$会直接输出4。在这个例子中，$W=[\begin{matrix}0.8&0&0.2\\0&1&0 \end{matrix}]$，其中第一行是$O_0$的输出，第二行是$O_1$的输出。

**Generating predictions**：说明不同思维类型的人会如何预测其他人的回答。这里的预测不是一个分布，而是一个其他人可能会报告的答案。当类型`t`的受访者预测时，她会运行一个预言机来得到预测结果$g\in A$，该预言机有概率$p_{t\rightarrow t'}$等于$O_{t'}$，其中$\sum_{t'}p_{t\rightarrow t'}=1$。

**Combination: answer-prediction joint distribution M**：`M`表示一个$|A|\times |A|$的矩阵，其中$M_{a,g}$是受访者回答`a`且预测他人回答`g`的概率，$\Lambda$表示一个$|T|\times |T|$的矩阵，其中$\Lambda_{t,t'}$是受访者为类型`t`且预测他人类型`t'`的概率。

**例2.3**：在本例中，当类型`0`的受访者进行预测时，她会以概率`1`再次运行预言机$O_0$。当类型`1`的受访者进行预测时，她会以`0.5`的概率运行预言机$O_0$，`0.5`的概率运行预言机$O_1$。且受访者有`0.7`的概率是类型`0`，`0.3`的概率是类型`1`。则$\Lambda=[\begin{matrix}p_0p_{0\rightarrow 0}&p_0p_{0\rightarrow 1}\\p_1p_{1\rightarrow 0}&p_1p_{1\rightarrow 1}\end{matrix}]=[\begin{matrix}0.7&0\\0.15&0.15\end{matrix}]$

**观点2.4**：基于以上生成过程，$M=W^{\top}\Lambda W$。
**证明**：对于每一个受访者，她回答`a`且预测`g`的概率为
$$M_{a, g}=\sum_t p_t \mathbf{w}_t(a) \sum_{t^{\prime}} p_{t \rightarrow t^{\prime}} \mathbf{w}_{t^{\prime}}(g)=\sum_{t, t^{\prime}} \mathbf{w}_t(a) p_t p_{t \rightarrow t^{\prime}} \mathbf{w}_{t^{\prime}}(g)$$
我们对所有受访者可能的类型进行求和，假定她的类型为`t`，则她会运行预言机$O_t$来得到答案，且有概率$w_t(a)$得到答案`a`。我们对她所有可能为了预测而运行的预言机进行求和，假定她运行$O_{t'}$，则预测为`g`的概率是$w_{t'}(g)$。

**关键假设**：上三角(`upper-triangular`)$\Lambda$. 我们假设，水平较低的人永远无法运行水平较高的预言机。类型$\pi:\lbrace 1,2,3,...,|T| \rbrace \mapsto T$的线性排序将排名位置映射到类型。例如，$\pi(1)\in T$是排序最高的类型。

**假设2.5**：我们假设，在类型的适当排序$\pi$下，$\Lambda$是一个上三角矩阵。形式上而言，存在$\pi$使得$\forall i > j, \Lambda_{\pi(i),\pi(j)}=0$。任意能使$\Lambda$为上三角形式的$\pi$都是这些类型的一种有效的思维层次。

在运行示例（例2.2）中，有效的思维层次是$\pi(1)$为类型1，$\pi(2)$为类型0。需要注意的是，在上面的假设中，并不要求$\forall i \leq j, \Lambda_{\pi(i),\pi(j)}>0$，当$\Lambda$为对角矩阵时，类型之间不能相互预测，而且同样复杂，因此任何排序都是有效的思维层次。

给定由未知的`w`和$\Lambda$生成的`M`，算法可以寻找其思维层次，并输出矩阵$W^\ast$，它等价于行顺序是有效思维层次的行置换后的`W`。形式上而言，存在一种有效的思维层次$\pi$使得$W^\ast$的第`i`行是$W$的第$\pi(i)$行，即$w_i^\ast=w_{\pi(i)}$。

### Non-negative Congruence Triangularization (NCT)
在上述模型中，推断思维层次引出了一个新的矩阵分解问题，这个问题类似于对称非负矩阵分解问题(NMF)。

**定义2.6**：非负同余三角化(NCT)。给定非负矩阵`M`，`NCT`旨在寻找非负矩阵`W`（是不止一个非负矩阵）和非负上三角矩阵$\Lambda$，使得$M=W^\top\Lambda W$。在一个基于 Frobenius 范数的近似版本中，给定矩阵`W`的集合，`NCT`旨在寻找非负矩阵`W`（是不止一个非负矩阵）和非负上三角矩阵$\Lambda$，从而最小化：
$$\min _{\mathbf{W} \in \mathcal{W}, \boldsymbol{\Lambda}}\left\|\mathbf{M}-\mathbf{W}^{\top} \boldsymbol{\Lambda} \mathbf{W}\right\|_F^2$$
得到的最小值被定义为：`M`关于$\mathcal{W}$的不适度(lack-of-fit)。

类似NMF，要求结果的严格唯一性是不可能的。令$P_{\Lambda}$表示能使得$\Pi^\top\Lambda\Pi$仍然是上三角的置换矩阵的集合。如果$(W,\Lambda)$是一个解，则当`D`是所有元素为正数的对角矩阵且$\Pi\in P_\Lambda$时，$(\Pi^{-1}DW, \Pi^\top D^{-1}\Lambda D^{-1}\Pi)$也是一个解。我们将唯一性结果陈述如下，并在附录C中证明。

**定理2.7**：唯一性。如果$|T|\leq |A|$，且`W`的`T`列包含一个置换正对角矩阵，则$M=W^\top\Lambda W$的`NCT`是唯一的，因为对于所有的$W'^\top\Lambda' W'=W^\top\Lambda W$，都存在一个正对角矩阵`D`和一个$|T|\times |T|$置换矩阵$\Pi\in P_\Lambda$，使得$W'=\Pi^{-1}DW$。

当我们将`W`限制为半正交时，就可以在不需要寻找最优$\Lambda$的情况下得到`NCT`的简洁格式。$\mathcal{I}$是所有半正交矩阵`W`的集合，其中`W`的每一列有且仅有一个非零元素，且$WW^\top=I$。例如，例2.2中的矩阵`W`可以被标准化为半正交矩阵，下面的引理来自于 Frobenius 范数的扩展，我们在附录C中进行证明。

**引理2.8**：半正交：最小F范数=最大化平方的上三角和。对于所有矩阵$\mathcal{W}\subset \mathcal{I}$的集合，$\min _{\mathbf{W} \in \mathcal{W}, \boldsymbol{\Lambda}}\left\|\mathbf{M}-\mathbf{W}^{\top} \boldsymbol{\Lambda} \mathbf{W}\right\|_F^2$等价于求解$max_{\mathbf{W} \in \mathcal{W}}\sum_{i\leq j}(\mathbf{W}\mathbf{M}\mathbf{W}^{\top})^2_{i,j}$，且设置$\Lambda$为$Up(\mathbf{W}\mathbf{M}\mathbf{W}^{\top})$，即$\mathbf{W}\mathbf{M}\mathbf{W}^{\top}$的上三角区域。

### Inferring the thinking hierarchy with answer-prediction joint distribution M
给定`M`，推断其思维层次，等价于求解`NCT`问题。虽然我们并没有`M`，但我们可以获取其代理。为了简化实际应用，我们基于引理2.8引入了两个简单的排序算法。排序算法以`M`为输入，输出所有回答的线性排序结果$\pi:\lbrace 1,2,...,|A| \rbrace \mapsto A$，它表示了排序位置到回答的映射。

**回答排序算法（默认）**：记作AR(M)，该算法计算
$$
\boldsymbol{\Pi}^* \leftarrow \arg \max _{\Pi \in \mathcal{P}} \sum_{i \leq j}\left(\boldsymbol{\Pi} \mathbf{M} \boldsymbol{\Pi}^{\top}\right)_{i, j}^2
$$
其中，$\mathcal{P}$是所有$|A|\times |A|$的置换矩阵的集合。对于所有`i`，在每一个置换矩阵$\Pi$和一个线性顺序$\pi:\Pi_{i,\pi(i)}=1$ 之间都存在一个一对一的映射。因此，最优$\Pi^\ast$会引出所有回答的最优排序，而默认算法也可以表示为：
$$
\pi^* \leftarrow \arg \max _\pi \sum_{i \leq j} M_{\pi(i), \pi(j)}^2
$$
默认算法隐含假设$|T|=|A|$，且所有预言机都是确定的。为了允许$|T|\leq|A|$和不确定的预言机，我们引入了一个变体，将$\mathcal{P}$推广到半正交矩阵$\mathcal{I}$的一个子集。每一个$|T|\times |A|$的半正交矩阵`W`表示一个硬聚类，每一类$t\in T$包含了所有使得$W_{t,a}>0$成立的回答。例如，例2.2中的矩阵`W`可以被标准化为半正交矩阵，并表示一个硬聚类`{4},{6,3}`。因此，变体算法将把回答划分为多个聚类，并为之分配一个层次结构。

**回答排序算法（变体）**：记作$AR^{+}(M,\mathcal{W})$，该算法计算
$$
\mathbf{W}^{*} \leftarrow \arg \max _{\mathbf{W} \in \mathcal{I}} \sum_{i \leq j}\left(\mathbf{W M W}^{\top}\right)_{i, j}^{2} 
$$
其中，$\mathcal{W}\subset \mathcal{I}$。$\mathbf{W}^{\ast}$被标准化为每行之和是1。

$\mathbf{W}^{\ast} \Rightarrow$ Answer rank  输出$\mathbf{W}^{\ast}$表示了所有回答的硬聚类。我们按照如下方式对所有回答进行排序：对于任意$i<j$，聚类`i`中的回答比聚类`j`中的回答有着更高的排序。对于所有`i`，任意两个聚类`i`中的回答`a`和`a'`，如果$W_{i,a}^\ast >W_{i,a'}^\ast$，则`a`排序高于`a'`。

**理论论证**：当`M`完全符合模型约束条件，即隐含的`W`为置换矩阵或硬聚类时，我们的算法就找到了思维层次。否则，我们的算法会找到由 Frobenius 范数度量的“最接近”的解。

**定理2.9**：当存在$\Pi_0 \in \mathcal{P}$和非负上三角矩阵$\Lambda_0$使得$\mathbf{M}=\boldsymbol{\Pi}_{0}^{\top} \boldsymbol{\Lambda}_{0} \boldsymbol{\Pi}_{0}$，算法AR(M)可以找到思维层次。一般来说，AR(M)会输出$\boldsymbol{\Pi}^\ast$，其中$\boldsymbol{\Pi}^\ast,\Lambda^\ast = Up(\boldsymbol{\Pi}^\ast M {\boldsymbol{\Pi}^\ast}^\top)$是$\arg \min _{\Pi \in \mathcal{P}, \Lambda}\left\|\mathbf{M}-\boldsymbol{\Pi}^{\top} \Lambda \Pi\right\|_{F}^{2}$的一个解。把$\mathcal{P}$换成$\mathcal{W}\subset \mathcal{I}$，把AR(M)换成$AR^{+}(M,\mathcal{W})$，上述语句仍然成立。

证明仍然在附录C。

### A proxy for answer-prediction joint distribution M
在实践中，我们没有完美的`M`。我们使用下面的开放式反应范式来获得`M`的代理。

**回答-预测范式**：受访者会被问两个问题：
1. 你的回答是什么？
2. 你认为其他人会回答什么？

在圆圈问题中，可能的反馈是：“回答4，预测3”、“回答3，预测6，9，1”等。我们用`A`表示所有回答的集合。在圆圈问题的例子中，$A=\lbrace 1,2,3,4,6,9 \rbrace$。我们同样允许受访者不进行预测或者预测多个值。

**回答-预测矩阵**：我们聚合反馈并通过回答-预测矩阵将其可视化。回答-预测矩阵`A`是一个$|A|\times |A|$的方阵，其中$|A|$是受访者提供的不同答案的数量。每一项$A_{a,g}, a,g\in A$是回答`a`且预测`g`的受访者的数量。

我们将证明，在适当的假设下，回答-预测矩阵`A`的期望与`M`成正比。首先，为了便于分析，我们假设每个受访者的预测都是独立样本。其次，由于我们允许人们有选择地提供预测，因此我们还需要假设每个受访者愿意提供的预测数量与她的类型和答案无关。我们将形式化的结果陈述如下，并在附录 C中进行证明。

**定理2.10**：当每一个受访者的预测都是独立样本，并且她给出的预测数量与她的类型和答案无关时，回答-预测矩阵`A` 的期望与`M`成正比。

## Studies
我们进行了四组研究，分别是：研究1(35道数学题) ，研究2(30道围棋题) ，研究3(44道常识题)和研究4(43道汉字发音题)。

**数据收集**：

**数据处理**：

### Results

## Discussion