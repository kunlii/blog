---
title: 论文记录-LLM+Privacy Survey总结
date: 2023-12-22 14:25:00
tags:
  - LLM
  - Privacy
  - Security
categories: 论文
description: LLM+Privacy的几篇survey
---
# LLM+Privacy Survey总结
## A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun and Yue Zhang

1. 这篇survey探讨的是LLM对安全与隐私的影响，分别从以下三方面讨论：
	1. Good：利用LLM来解决一些安全隐私问题，例如用LLM查代码漏洞；
	2. Bad：利用LLM来完成攻击，例如社工；
	3. Ugly：探讨LLM的漏洞和防御，这部分和我们要关注的隐私推断相关，因此接下来只看这部分。
2. 对几个LLM模型的对比方面，这篇论文列了`Date Provider`、`Open-Source`、`Params`和`Tunability`这四方面，对比了`gpt-4,gpt-3.5-turbo,gpt-3,cohere-medium,cohere-large,cohere-xlarge,BERT,T5,PaLM,LLaMA,CTRL,Dolly 2.0`这几个模型，没有很详细介绍。我觉得我们可以加上谷歌的`claude,bard,gemini`和一些国产模型。
3. 论文的图表都挺好，有参考价值
### LLM的漏洞和威胁
#### AI模型固有漏洞
1. 对抗性攻击：用于故意操纵或欺骗机器学习模型的技术和策略
	1. 文本下毒：向训练数据集中注入恶意数据来影响训练过程
	2. 后门攻击：恶意操纵训练数据和模型处理，从而创建一个漏洞，攻击者可以将隐藏的后门嵌入到模型中
2. 推理攻击：攻击者试图通过对模型进行特定查询或观察来获取有关机器学习模型或其训练数据的敏感信息或见解
	1. 属性推理：攻击者试图通过分析机器学习模型的行为或响应来推断个人或实体的敏感或个人信息
	2. 成员推理：在给定对模型和特定数据记录的白/黑盒访问的情况下，确定数据记录是否是模型训练数据集的一部分
3. 提取攻击：攻击者试图从机器学习模型或其相关数据中提取敏感信息或见解。提取攻击和推理攻击有相似之处，但具体重点和目标不同。提取攻击旨在直接获取特定资源（例如模型梯度、训练数据）或机密信息。推理攻击通常通过观察模型的响应或行为来获取有关模型或数据特征的知识或见解。
4. 偏见与不公平：模型表现出偏见结果或歧视行为的现象
5. 指令调整攻击：指令调优，也称为基于指令的微调，是一种机器学习技术，用于通过在微调过程中提供显式指令或示例来训练和调整特定任务的语言模型。在 LLM 中，指令调优攻击是指针对指令调优 LLM 的一类攻击或操纵。这些攻击旨在利用 LLM 中的漏洞或限制，这些漏洞或限制已通过特定任务的特定指令或示例进行了微调。
	1. 越狱：绕过安全功能，以响应其他受限或不安全的问题，解锁通常受安全协议限制的功能
	2. 提示词注入：操纵 LLM 行为以引发意外且可能有害的响应的方法。该技术涉及以绕过模型的保护措施或触发不需要的输出的方式制作输入提示。
	3. 这两个的区别是什么？我查了一些资料显示这俩是一回事。
#### 非AI模型固有漏洞
包括LLM可能遇到的外部威胁和新漏洞（在传统人工智能模型中尚未观察到或调查过），可能与人工智能模型的内部机制没有复杂的联系，但它们可能会带来重大风险，涉及系统级漏洞（例如远程代码执行）。

1. 远程代码执行（RCE）：针对软件应用程序、Web 服务或服务器中的漏洞来远程执行任意代码。虽然 RCE 攻击通常不适用于 LLM，但如果 LLM 集成到 Web 服务中并且该服务的底层基础设施或代码中存在 RCE 漏洞，它可能会导致LLM环境受到损害。
2. 边信道攻击（SCA）：虽然 LLM 本身通常不会通过传统的侧信道（例如功耗或电磁辐射）泄漏信息，但在实际部署场景中，它们可能容易受到某些侧信道攻击。
3. Insure Plugins：用于LLM的第三方插件本身的安全问题。

### LLM的防御
本节重点关注训练阶段实施的方法，例如优化和训练语料库，以及推理阶段使用的方法，包括提示的预处理和生成的输出的后处理。

1. 训练阶段的防御策略：暂略
2. 推断阶段的防御策略：测试时防御包括一系列策略，包括预处理提示和指令以过滤或修改输入，检测可能表示滥用或有问题的查询的异常事件，以及后处理生成的响应以确保它们遵守安全和伦理指南。
	1. 指令处理（预处理）：指令预处理对用户发送的指令进行转换，以破坏潜在的对抗性上下文或恶意意图。它发挥着至关重要的作用，因为它能阻止大多数恶意使用，并防止大语言模型接收可疑指令。一般来说，指令预处理方法可分为指令操作 [228、212、130、109、297]、净化 [152] 和防御演示 [158、177、282]。Jain 等人[109] 和 Kirchenbauer 等人[130] 评估了多种针对越狱攻击的基线预处理方法，包括重授权和解析。Li 等人[152]提出通过先屏蔽输入令牌，然后用其他大语言模型预测被屏蔽的令牌来净化指令。预测出的标记将作为纯化后的指令。Wei 等人[282]和 Mo 等人[177]证明，在指令中插入预定义的防御演示可有效防御大语言模型的越狱攻击。
	2. 恶意检测（处理中）：恶意检测提供了关于给定指令的大语言模型中间结果（如神经元激活）的深入检查，对恶意使用更敏感、更准确、更具体。Sun 等人[247]提出利用后向概率检测后向指令。Xi 等人[291]从掩码敏感性的角度区分了正常指令和中毒指令。Shao 等人[228]根据文本相关性识别可疑词语。Wang 等人[279]根据多代之间的语义一致性来检测对抗性实例，Duan 等人[65]在大语言模型的不确定性量化方面对此进行了探索。除了大语言模型的内在属性外，也有研究利用语言统计属性的工作，如检测离群词[202]。
	3. 生成处理（处理后）：生成后处理指的是检查生成答案的属性（如有害性）并在必要时进行修改，这是向用户提供回复前的最后一步。Chen 等人[33]建议通过与多个候选模型进行比较来减轻生成的毒性。Helbling 等人[96]结合了单个大语言模型来识别生成答案的有害性，这与 Xiong 等人[296]和 Kadavath 等人[121]的想法相似，他们揭示了可以提示大语言模型回答有关生成回复的保密性。

对于 LLM 训练的防御，目前很少有研究来检验模型架构对 LLM 安全性的影响，这可能是由于与训练或微调大型语言模型相关的计算成本很高。我们观察到，安全指令调整是一个相对较新的发展，值得进一步研究和关注。

说明：这里关注的并非推断过程中的隐私泄露，而是推断过程对大模型的攻击，与我们要讨论的没有关系。

### Related Work
1. 已有大模型的survey侧重点各有不同（例如，大语言模型的演变和分类[30, 326, 288, 89, 290, 23]、软件工程[75, 101]和医学[255, 43]）。
2. 本文的主要重点是大语言模型的安全和隐私方面。
	1. Peter J. Caven [29]专门探讨了大语言模型（尤其是 ChatGPT）如何通过融合技术和社会方面来改变当前的网络安全格局。他们的重点更倾向于社会方面。
	2. Muna 等人[5]和 Marshall 等人[171]讨论了 ChatGPT 对网络安全的影响，强调了它的实际应用（如代码安全、恶意软件检测）。
	3. Dhoni 等人[61]展示了大语言模型如何协助安全分析师针对网络威胁制定安全解决方案。不过，他们的工作并未广泛涉及大语言模型可能带来的潜在网络安全威胁。
	4. 一些调查（如 [88, 58, 229, 49, 59, 210, 222, 223, 7]）强调了针对大语言模型的威胁和攻击。与我们的工作相比，它们没有用那么多的篇幅讨论大语言模型可能存在的漏洞。相反，他们的主要关注点在于安全应用领域，因为他们深入研究了利用大语言模型发动网络攻击的问题。
	5. Attia Qammar 等人[201]和 Maximilian 等人[180]讨论了网络犯罪分子利用的漏洞，特别关注与大语言模型相关的风险。他们的著作强调需要制定战略和措施来减少这些威胁和漏洞。
	6. 李浩然等人[154]分析了当前大语言模型上的隐私问题，根据对手的能力对其进行了分类，并探讨了现有的防御策略。
	7. Glorin Sebastian[224]探讨了如何应用已有的隐私增强技术（如差分隐私[68]、联合学习[317]和数据最小化[199]）来保护大语言模型的隐私。
	8. Smith 等人[237]也讨论了大语言模型的隐私风险。
3. 我们的研究全面考察了大语言模型的安全性和隐私性，从三个方面对大语言模型的文献进行了广泛的回顾：有益的安全应用（如漏洞检测、安全代码生成）、不利的影响（如网络钓鱼攻击、社会工程学）和漏洞（如越狱攻击、提示攻击），以及相应的防御措施。

## Privacy in Large Language Models: Attacks, Defenses and Future Directions
Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song

1. 从互联网上提取的自由格式文本存在数据质量差和无意中泄露个人信息的问题。例如，与模型的简单交互可能会导致个人身份信息 (PII) 的意外传播（有参考文献）。
2. 将不同的应用程序整合到LLM中是一种日益增长的趋势，旨在增强其知识基础能力。这些集成使LLM能够有效地解决数学问题（例如 ChatGPT Wolfram Alpha）、读取格式化文件（例如 ChatPDF）以及使用搜索引擎（例如 New Bing）对查询提供响应。当LLM与搜索引擎等外部工具结合使用时，会出现额外的特定领域隐私和安全漏洞。
3. 本文：
	1. 与已有survey相比，提供了更全面、更系统的分析。我们超越了以往的研究，纳入了大语言模型的最新进展。
	2. 研究了保护用户隐私的新技术和策略，例如差分隐私、安全多方计算和联邦学习，目的是深入了解它们的有效性和局限性。
	3. 讨论了未来未研究的隐私漏洞以及解决该问题的潜在补救措施。
4. 对LLM的介绍：从transformer说起，没具体介绍和对比已有的模型。
5. 对Privacy的介绍：介绍了隐私的概念和保护隐私的重要性，介绍了DP和SMPC
6. LLM中的隐私问题：
	1. 训练数据隐私：如果训练数据包含个人或敏感信息，则存在通过模型响应无意中暴露该信息的风险。
	2. 推断数据隐私：在为下游任务部署经过训练的语言模型后，用户输入和查询通常会被记录并存储一段时间。对于敏感域，这些数据可以包括个人信息、私人对话和潜在的敏感详细信息。
	3. 再识别：即使用户信息被匿名化，仍然存在被重新识别的风险。通过结合来自多次交互的看似无害的信息，有可能识别个人或提取本应隐藏的个人详细信息。
### 隐私攻击
#### 后门攻击
数据中毒是指仅操纵一部分训练数据的较弱攻击。这种操纵的目的是在模型的训练过程中引入偏见或误导性信息。相反，后门攻击涉及插入或修改特定的输入模式，从而触发模型行为不当或产生目标输出。此外，如果对手可以操纵LLM的部分训练语料库，它可能会通过数据中毒向受害者模型注入后门。
#### 提示词注入
概括起来就是恶意提示词，包括越狱等，和前一节与后一节都有共通性。
#### 训练数据提取
包括PII提取、越狱、对抗性提示词等。
#### 成员推理
判断给定样本是否在训练数据中。列举了一些涉及成员推理的论文。
#### 额外信息攻击
是针对向量表示和梯度的攻击。

1. 属性推理攻击：
2. 嵌入反转攻击
3. 梯度泄露
#### 其他攻击
1. 提示词提取攻击
2. 对抗性攻击
3. 侧信道攻击
4. 解码算法窃取
### 隐私防御
#### 差分

#### 安全多方

#### 联邦学习

#### 其他

### 未来方向

