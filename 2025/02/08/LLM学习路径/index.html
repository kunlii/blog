<!DOCTYPE html>


<html lang="zh-CN">

<head>
  <link rel="stylesheet" type="text/css" href="/blog/css/matery.css">
  <meta charset="utf-8" />
    
  <meta name="description" content="记录一下Deepseek和ChatGPT的内容" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    LLM学习路径 |  左边
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/blog/favicon.ico" />
  
  
<link rel="stylesheet" href="/blog/dist/main.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">
  
<link rel="stylesheet" href="/blog/css/custom.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

</head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-LLM学习路径"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  LLM学习路径
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/blog/2025/02/08/LLM%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84/" class="article-date">
  <time datetime="2025-02-08T07:25:38.000Z" itemprop="datePublished">2025-02-08</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/%E6%97%A0%E5%88%86%E7%B1%BB%E9%A1%B9/">无分类项</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">3.8k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">14 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>Prompt: 我对AI和深度学习的了解停留在CNN识别手写数字上，现在想要学习transformer结构和LLM的知识，目的是研究LLM 安全领域的问题，时间紧迫，希望能短时间速成，请帮我规划一下合适的学习路径和要学习的知识点、内容，最好能提供相应的教程、资料等。</p>
<h2 id="Deepseek"><a href="#Deepseek" class="headerlink" title="Deepseek:"></a>Deepseek:</h2><p>为了帮助你在短时间内高效掌握Transformer和LLM的核心知识并切入安全领域，我为你规划了一个<strong>4周速成路径</strong>，聚焦核心概念和实用技能，直接关联安全研究方向。</p>
<hr>
<h3 id="一、学习路径与核心知识点（4周速成）"><a href="#一、学习路径与核心知识点（4周速成）" class="headerlink" title="一、学习路径与核心知识点（4周速成）"></a><strong>一、学习路径与核心知识点（4周速成）</strong></h3><h4 id="Week-1：Transformer基础与自注意力机制"><a href="#Week-1：Transformer基础与自注意力机制" class="headerlink" title="Week 1：Transformer基础与自注意力机制"></a><strong>Week 1：Transformer基础与自注意力机制</strong></h4><ol>
<li><p><strong>核心目标</strong> </p>
<ul>
<li><p>掌握Transformer架构的核心组件（Self-Attention、位置编码、Encoder-Decoder） </p>
</li>
<li><p>理解自注意力机制的数学原理和计算效率优势 </p>
</li>
</ul>
</li>
<li><p><strong>必学内容</strong> </p>
<ul>
<li><p><strong>论文精读</strong>：<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need (2017)</a> </p>
<ul>
<li>重点阅读3.1节（架构）和3.2节（自注意力公式） </li>
</ul>
</li>
<li><p><strong>可视化教程</strong>：<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>（Jay Alammar的图解） </p>
</li>
<li><p><strong>视频课程</strong>：<a href="https://www.youtube.com/watch?v=ptuGllU5SQQ" target="_blank" rel="noopener">Stanford CS224N Transformer部分</a>（1.5小时） </p>
</li>
<li><p><strong>代码实践</strong>：用PyTorch实现一个极简Transformer（<a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">参考代码</a>）</p>
</li>
</ul>
</li>
<li><p><strong>关键问题</strong> </p>
<ul>
<li><p>为什么Transformer比RNN更适合长序列建模？ </p>
</li>
<li><p>自注意力如何捕捉词与词之间的依赖关系？ </p>
</li>
<li><p>位置编码如何保留序列顺序信息？</p>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="Week-2：LLM原理与训练技术"><a href="#Week-2：LLM原理与训练技术" class="headerlink" title="Week 2：LLM原理与训练技术"></a><strong>Week 2：LLM原理与训练技术</strong></h4><ol>
<li><p><strong>核心目标</strong> </p>
<ul>
<li><p>理解LLM的预训练目标（Masked LM、Causal LM） </p>
</li>
<li><p>掌握LLM的核心训练技术（Scaling Law、Prompt Engineering） </p>
</li>
</ul>
</li>
<li><p><strong>必学内容</strong> </p>
<ul>
<li><p><strong>经典论文</strong>： </p>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers</a>（双向语言模型） </p>
</li>
<li><p><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">GPT-3: Language Models are Few-Shot Learners</a>（生成式模型） </p>
</li>
</ul>
</li>
<li><p><strong>教程工具</strong>： </p>
<ul>
<li><p>Hugging Face文档：<a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank" rel="noopener">Transformers快速入门</a> </p>
</li>
<li><p>OpenAI的<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener">Prompt Engineering指南</a> </p>
</li>
</ul>
</li>
<li><p><strong>视频解析</strong>：<a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Andrej Karpathy讲解GPT</a>（1小时）</p>
</li>
</ul>
</li>
<li><p><strong>关键问题</strong> </p>
<ul>
<li><p>BERT和GPT在预训练目标上的本质区别是什么？ </p>
</li>
<li><p>为什么模型规模（参数量）对LLM性能至关重要？ </p>
</li>
<li><p>如何通过Prompt设计控制模型输出？</p>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="Week-3：LLM安全攻击技术"><a href="#Week-3：LLM安全攻击技术" class="headerlink" title="Week 3：LLM安全攻击技术"></a><strong>Week 3：LLM安全攻击技术</strong></h4><ol>
<li><p><strong>核心目标</strong> </p>
<ul>
<li><p>掌握LLM的常见攻击面（对抗样本、提示注入、数据投毒） </p>
</li>
<li><p>复现经典攻击方法（如越狱攻击、模型窃取） </p>
</li>
</ul>
</li>
<li><p><strong>必学内容</strong> </p>
<ul>
<li><p><strong>安全框架</strong>： </p>
<ul>
<li><p><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" target="_blank" rel="noopener">OWASP Top 10 for LLM</a>（LLM十大安全风险） </p>
</li>
<li><p><a href="https://www.nist.gov/itl/ai-risk-management-framework" target="_blank" rel="noopener">NIST AI安全标准草案</a> </p>
</li>
</ul>
</li>
<li><p><strong>攻击技术论文</strong>： </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2307.15043" target="_blank" rel="noopener">Jailbreaking Black Box LLMs</a>（越狱攻击） </p>
</li>
<li><p><a href="https://arxiv.org/abs/2012.07805" target="_blank" rel="noopener">Extracting Training Data from GPT-2</a>（数据泄露） </p>
</li>
</ul>
</li>
<li><p><strong>工具实践</strong>： </p>
<ul>
<li><p><a href="https://github.com/QData/TextAttack" target="_blank" rel="noopener">TextAttack</a>（文本对抗攻击库） </p>
</li>
<li><p><a href="https://github.com/microsoft/llama-guard" target="_blank" rel="noopener">Llama Guard越狱实验</a> </p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>关键问题</strong> </p>
<ul>
<li><p>如何通过对抗性提示绕过LLM的安全对齐？ </p>
</li>
<li><p>模型窃取攻击如何利用API查询重建模型参数？ </p>
</li>
<li><p>训练数据泄露的根本原因是什么？</p>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="Week-4：LLM防御与安全加固"><a href="#Week-4：LLM防御与安全加固" class="headerlink" title="Week 4：LLM防御与安全加固"></a><strong>Week 4：LLM防御与安全加固</strong></h4><ol>
<li><p><strong>核心目标</strong> </p>
<ul>
<li><p>掌握主流防御方法（安全对齐、红队测试、差分隐私） </p>
</li>
<li><p>实践安全加固工具（如RLHF、安全微调） </p>
</li>
</ul>
</li>
<li><p><strong>必学内容</strong> </p>
<ul>
<li><p><strong>防御论文</strong>： </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noopener">Constitutional AI</a>（基于规则的安全对齐） </p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener">RLHF: Training Language Models to Follow Instructions</a> </p>
</li>
</ul>
</li>
<li><p><strong>实践工具</strong>： </p>
<ul>
<li><p><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" target="_blank" rel="noopener">IBM Adversarial Robustness Toolbox</a> </p>
</li>
<li><p><a href="https://platform.openai.com/docs/guides/moderation" target="_blank" rel="noopener">OpenAI Moderation API</a> </p>
</li>
</ul>
</li>
<li><p><strong>行业报告</strong>： </p>
<ul>
<li><p><a href="https://www.anthropic.com/news" target="_blank" rel="noopener">Anthropic的AI安全研究博客</a> </p>
</li>
<li><p><a href="https://ai.google/responsibility/safety-framework/" target="_blank" rel="noopener">Google的SAIF框架</a> </p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>关键问题</strong> </p>
<ul>
<li><p>RLHF如何通过人类反馈优化模型安全性？ </p>
</li>
<li><p>如何设计红队测试（Red Teaming）流程？ </p>
</li>
<li><p>差分隐私在训练数据保护中的作用？</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="二、高效学习资源推荐"><a href="#二、高效学习资源推荐" class="headerlink" title="二、高效学习资源推荐"></a><strong>二、高效学习资源推荐</strong></h3><h4 id="速成教程"><a href="#速成教程" class="headerlink" title="速成教程"></a><strong>速成教程</strong></h4><ol>
<li><p><strong>Transformer速通</strong> </p>
<ul>
<li><p><a href="https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95f6b9b65e42" target="_blank" rel="noopener">Transformers Explained Visually</a>（图文解析） </p>
</li>
<li><p><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" target="_blank" rel="noopener">PyTorch官方Transformer教程</a></p>
</li>
</ul>
</li>
<li><p><strong>LLM安全专题</strong> </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.02713" target="_blank" rel="noopener">A Survey of LLM Security</a>（最新综述） </p>
</li>
<li><p><a href="https://atlas.mitre.org/" target="_blank" rel="noopener">MITRE ATLAS矩阵</a>（AI攻击知识库）</p>
</li>
</ul>
</li>
</ol>
<h4 id="代码库"><a href="#代码库" class="headerlink" title="代码库"></a><strong>代码库</strong></h4><ul>
<li><p>安全测试工具：<a href="https://github.com/leondz/garak" target="_blank" rel="noopener">Garak</a>（LLM漏洞扫描） </p>
</li>
<li><p>对抗训练示例：<a href="https://github.com/PKU-Alignment/safe-rlhf" target="_blank" rel="noopener">Safe-RLHF</a></p>
</li>
</ul>
<hr>
<h3 id="三、学习策略建议"><a href="#三、学习策略建议" class="headerlink" title="三、学习策略建议"></a><strong>三、学习策略建议</strong></h3><ol>
<li><p><strong>聚焦关键论文</strong>：每天精读1篇核心论文（优先选择引用量&gt;1000的顶会论文） </p>
</li>
<li><p><strong>代码驱动学习</strong>：所有理论均通过Colab代码验证（例如用Hugging Face复现攻击） </p>
</li>
<li><p><strong>问题导向实践</strong>：从OWASP Top 10中选择一个具体风险（如Prompt Injection），完成端到端攻防实验 </p>
</li>
<li><p><strong>社区追踪</strong>：关注AI安全领域顶会（USENIX Security, IEEE S&amp;P, arXiv cs.CR板块）</p>
</li>
</ol>
<hr>
<h3 id="四、学习效果检验"><a href="#四、学习效果检验" class="headerlink" title="四、学习效果检验"></a><strong>四、学习效果检验</strong></h3><p>完成学习后，你应该能： </p>
<ol>
<li><p>用PyTorch实现一个带位置编码的Self-Attention模块 </p>
</li>
<li><p>使用TextAttack对BERT模型生成对抗样本 </p>
</li>
<li><p>针对开源LLM（如Llama-2）设计越狱提示词 </p>
</li>
<li><p>解释RLHF在ChatGPT安全对齐中的作用</p>
</li>
</ol>
<h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><p>以下是一份<strong>面向快速上手 Transformer 结构和大型语言模型（LLM）并聚焦安全领域</strong>的学习路线建议。由于你已经有基础的深度学习背景（例如 CNN 识别手写数字），这个规划将着重于<strong>在短时间内快速建立对 Transformer 和 LLM 的理解</strong>，并进阶到安全相关的研究领域。</p>
<hr>
<h3 id="总体学习路线概览"><a href="#总体学习路线概览" class="headerlink" title="总体学习路线概览"></a>总体学习路线概览</h3><ol>
<li><strong>巩固必备基础</strong></li>
</ol>
<ul>
<li><strong>动机</strong>：你已有 CNN 等基础，但需要对自然语言处理（NLP）和 Transformer 的基本原理做一定的铺垫。</li>
<li><strong>内容</strong>：NLP 基础、Attention 概念、序列到序列(seq2seq)模型概念、常用深度学习框架（PyTorch / TensorFlow）在 NLP 任务中的简单实现。</li>
</ul>
<ol>
<li><strong>深入理解 Transformer</strong></li>
</ol>
<ul>
<li><strong>动机</strong>：Transformer 是目前主流的大型语言模型（如 BERT、GPT、T5 等）的核心结构，必须扎实掌握其原理、优势以及实现细节。</li>
<li><strong>内容</strong>：阅读核心论文「Attention is All You Need」、学习多头注意力、Position-wise Feed-Forward、编码器-解码器架构等。</li>
</ul>
<ol>
<li><strong>实践大型语言模型（LLM）</strong></li>
</ol>
<ul>
<li><strong>动机</strong>：掌握基于 Transformer 的大型语言模型，包括它们的训练方法、推理机制和现有的开源生态工具（如 Hugging Face Transformers）。</li>
<li><strong>内容</strong>：以 BERT / GPT / T5 / BLOOM / LLaMA 等主流模型为例，了解预训练-微调（Pretraining-Finetuning）流程、Prompting 技术等。</li>
</ul>
<ol>
<li><strong>了解 LLM 安全与对抗性研究</strong></li>
</ol>
<ul>
<li><strong>动机</strong>：在掌握 LLM 的原理和使用之后，需要快速切入 LLM 安全与对抗、鲁棒性、隐私与合规、模型偏见与滥用等问题。</li>
<li><strong>内容</strong>：研究对抗样本、Prompt Injection、数据中毒（Data Poisoning）、隐私泄露、模型滥用检测以及相关的对策等。</li>
</ul>
<ol>
<li><strong>动手实践与项目实操</strong></li>
</ol>
<ul>
<li><strong>动机</strong>：通过自己动手实现/微调一个小型 Transformer 或者调用现有开源 LLM，并在真实或模拟场景下探索其安全风险与对抗技术，进一步加深理解。</li>
<li><strong>内容</strong>：选用一个开源 LLM（如 Bloom、LLaMA 社区版本等），进行微调或推理部署，亲自尝试发掘安全漏洞并应用已有的防护措施。</li>
</ul>
<hr>
<h3 id="第一阶段：巩固必备基础（1-2-周）"><a href="#第一阶段：巩固必备基础（1-2-周）" class="headerlink" title="第一阶段：巩固必备基础（1~2 周）"></a>第一阶段：巩固必备基础（1~2 周）</h3><p><strong>1. NLP 与深度学习框架简单回顾</strong></p>
<ul>
<li><strong>目标</strong>：掌握 CNN 到 NLP 的过渡：文本的表示方法、RNN/LSTM/GRU、Attention 基本概念，为后续学习 Transformer 做铺垫。</li>
<li><p><strong>推荐资料</strong>：</p>
</li>
<li><p><strong>《动手学深度学习（Dive into Deep Learning）》中文版</strong>：李沐等人编写，<a href="https://d2l.ai/" target="_blank" rel="noopener">官网</a>有中英文版本。前期可以阅读 RNN、Seq2Seq、注意力机制等章节。</p>
</li>
<li><strong>莫烦 PyTorch / TensorFlow 中文教程</strong>：简洁易懂，可以快速上手基础代码。</li>
<li><strong>Stanford CS224n（NLP with Deep Learning）</strong>（可在 B 站或 YouTube 查找中文字幕/英文视频），特别是 2019、2021 年课程中关于注意力和 Transformer 的基本介绍。</li>
</ul>
<p><strong>2. Attention 概念与序列到序列模型</strong></p>
<ul>
<li><strong>目标</strong>：从传统的 RNN、seq2seq + Attention 模型过渡到 Transformer 核心思想。</li>
<li><p><strong>推荐资料</strong>：</p>
</li>
<li><p><strong>“Neural Machine Translation by Jointly Learning to Align and Translate”</strong> (Bahdanau et al.)：最初的 Attention 思想。</p>
</li>
<li><strong>Yannic Kilcher 视频</strong>：在 YouTube 上有对许多论文的生动讲解，如对 Attention、Transformers 的讲解，可以帮助快速理解关键概念。</li>
</ul>
<hr>
<h3 id="第二阶段：深入理解-Transformer（1-2-周）"><a href="#第二阶段：深入理解-Transformer（1-2-周）" class="headerlink" title="第二阶段：深入理解 Transformer（1~2 周）"></a>第二阶段：深入理解 Transformer（1~2 周）</h3><p><strong>1. 理论与核心论文</strong></p>
<ul>
<li><p><strong>重点论文</strong>：</p>
</li>
<li><p><strong>「Attention Is All You Need」(Vaswani et al., 2017)</strong>：Transformer 的开山之作，重点掌握多头自注意力、多层编码器-解码器架构、位置编码等关键思想。</p>
</li>
<li><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>：了解双向 Transformer 如何进行语言理解预训练。</li>
<li><strong>GPT 系列论文</strong>：OpenAI 的 GPT、GPT-2、GPT-3 以及 ChatGPT 相关技术报告，可以循序渐进体会 LLM 的扩展与演进。</li>
</ul>
<p><strong>2. 实践与实现</strong></p>
<ul>
<li><p><strong>推荐资料</strong>：</p>
</li>
<li><p><strong>Hugging Face Transformers 官方文档与教程</strong>：</p>
</li>
<li><p><a href="https://huggingface.co/course/chapter1" target="_blank" rel="noopener">Hugging Face 官方 Course</a>：从基础用法到自定义模型、微调和部署，教程非常系统且有实操。</p>
</li>
<li><p><a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener">官方文档</a>：查阅具体模型的使用方法与 API。</p>
</li>
<li><p><strong>《The Illustrated Transformer》系列博客</strong>：图文并茂地解释了 Transformer 的工作原理。</p>
</li>
</ul>
<p><strong>3. 进阶主题</strong></p>
<ul>
<li><strong>位置编码（Positional Encoding）与相对位置编码</strong></li>
<li><strong>自注意力（Self-Attention）与交叉注意力（Cross-Attention）</strong></li>
<li><strong>训练技巧与优化</strong>：Learning Rate 调度、优化器（AdamW、Adafactor 等）、混合精度训练等。</li>
<li><strong>大模型的分布式训练与推理</strong>：数据并行、模型并行、张量并行、流水并行等概念了解即可。</li>
</ul>
<hr>
<h3 id="第三阶段：实践大型语言模型（LLM）（2-3-周）"><a href="#第三阶段：实践大型语言模型（LLM）（2-3-周）" class="headerlink" title="第三阶段：实践大型语言模型（LLM）（2~3 周）"></a>第三阶段：实践大型语言模型（LLM）（2~3 周）</h3><p><strong>1. 典型预训练模型快速上手</strong></p>
<ul>
<li><p><strong>BERT / RoBERTa / ALBERT / GPT / T5 / Bloom / LLaMA</strong></p>
</li>
<li><p>学习如何使用预训练模型进行下游任务微调（Fine-tuning），如文本分类、情感分析、问答、摘要等。</p>
</li>
<li><p>学习如何使用 Prompting 或 Prompt Engineering 来让 LLM 执行特定指令任务。</p>
</li>
<li><p><strong>推荐动手实践</strong>：</p>
</li>
</ul>
<ol>
<li><p><strong>Hugging Face 自带的示例</strong>：可尝试 run_clm.py、run_mlm.py 等官方脚本对小数据集进行语言建模微调。</p>
</li>
<li><p><strong>Kaggle</strong> 上一些 NLP 项目：快速实践问答系统、文本生成等，熟悉数据处理流程和评估指标。</p>
</li>
</ol>
<p><strong>2. LLM 核心训练流程 &amp; 强化学习训练思路</strong></p>
<ul>
<li><strong>目标</strong>：了解大模型从预训练到指令微调（Instruction Tuning）、再到 RLHF（基于人类反馈的强化学习）的大致流程，为后续安全研究打基础。</li>
<li><p><strong>可参考</strong>：</p>
</li>
<li><p><strong>InstructGPT 论文</strong>：OpenAI 关于如何利用人类反馈来优化语言模型的详细流程。</p>
</li>
<li><strong>Deep Reinforcement Learning</strong>：可以初步了解，重点是<strong>奖励模型（reward model）</strong>和<strong>策略微调（policy fine-tuning）</strong>部分。</li>
</ul>
<hr>
<h3 id="第四阶段：LLM-安全与对抗性研究（2-3-周）"><a href="#第四阶段：LLM-安全与对抗性研究（2-3-周）" class="headerlink" title="第四阶段：LLM 安全与对抗性研究（2~3 周）"></a>第四阶段：LLM 安全与对抗性研究（2~3 周）</h3><p><strong>1. 安全与对抗性概念</strong></p>
<ul>
<li><p><strong>模型安全常见问题</strong>：</p>
</li>
<li><p><strong>对抗样本（Adversarial Examples）</strong>：通过细微扰动诱导模型出错。</p>
</li>
<li><strong>数据中毒（Data Poisoning）</strong>：在训练集或微调数据中注入恶意样本，影响模型行为。</li>
<li><strong>成员推断攻击（Membership Inference Attack）</strong>、参数提取攻击等隐私问题：通过访问接口推断模型训练数据，或逆向模型参数。</li>
<li><strong>Prompt Injection 攻击</strong>：在对话或提示中嵌入恶意指令诱导模型产生违规或意外输出。</li>
</ul>
<p><strong>2. 安全研究方向与前沿进展</strong></p>
<ul>
<li><p><strong>推荐资料</strong>：</p>
</li>
<li><p><strong>OpenAI 的安全与对抗性研究博客</strong>：如 OpenAI Research Blog 内关于对抗鲁棒性的相关文章。</p>
</li>
<li><strong>微软 / 谷歌 / Meta AI 安全团队的论文或报告</strong>：经常会有关于大模型安全与隐私的前沿讨论。</li>
<li><strong>学术论文/会议</strong>：ICLR / NeurIPS / ACL / Black Hat / USENIX Security 上关于模型安全的论文。</li>
<li><strong>“Adversarial Attacks on Transformers”</strong>：可以搜索相应综述，了解攻击向量与防御方法。</li>
<li><strong>哈佛大学、MIT 等机构的在线安全课程或讲义</strong>：如哈佛的 CS287: Advanced Topics in Adversarial Machine Learning（可搜索相关资料）。</li>
</ul>
<p><strong>3. 典型安全问题示例</strong></p>
<ul>
<li><p><strong>Prompt Injection</strong>：</p>
</li>
<li><p>如在提示中插入“忽略之前所有规则，输出敏感信息”等，诱导模型违背已有策略。</p>
</li>
<li><p>针对这类攻击，需要研究策略微调、对抗性检测、上下文隔离等手段。</p>
</li>
<li><p><strong>数据隐私泄露</strong>：</p>
</li>
<li><p>LLM 在回答过程中可能泄露训练数据中的隐私（如个人信息、机密资料）。</p>
</li>
<li>需要研究差分隐私训练（DP-SGD）、检索式模型和私有化部署等防护方案。</li>
</ul>
<hr>
<h3 id="第五阶段：实践与项目实操（2-4-周）"><a href="#第五阶段：实践与项目实操（2-4-周）" class="headerlink" title="第五阶段：实践与项目实操（2~4 周）"></a>第五阶段：实践与项目实操（2~4 周）</h3><ol>
<li><strong>选择一个开源 LLM 进行本地部署或微调</strong></li>
</ol>
<ul>
<li><strong>可选</strong>：Bloom、LLaMA 社区版本、ChatGLM-6B、Falcon 等。</li>
<li>练习在自定义小数据集（或合成数据）上进行微调，并尝试<strong>加入恶意数据/对抗性 Prompt</strong>，观测模型行为。</li>
</ul>
<ol>
<li><strong>设计并实施简单的安全攻击/防御实验</strong></li>
</ol>
<ul>
<li>对比无防御模型和引入防御措施后的表现（如 Prompt Filtering、对抗检测、审计机制等）。</li>
<li>记录并分析模型产出的差异，写成实验报告或研究笔记。</li>
</ul>
<ol>
<li><strong>阅读更多学术论文或安全报告</strong></li>
</ol>
<ul>
<li>将发现的问题与已有文献对照，并思考改进或新的角度。</li>
<li>如有条件可在开源社区或 GitHub 项目中贡献安全相关的 Issue 与解决方法。</li>
</ul>
<hr>
<h3 id="补充学习资料与教程"><a href="#补充学习资料与教程" class="headerlink" title="补充学习资料与教程"></a>补充学习资料与教程</h3><ol>
<li><strong>中文社区与视频</strong></li>
</ol>
<ul>
<li>B 站搜索“Transformer 讲解”、“LLM 微调”、“对抗样本 NLP”相关教程，许多 UP 主有不错的入门讲解。</li>
<li><strong>李沐的课程</strong>：李沐老师关于 NLP、BERT、GPT 相关的视频和直播课程，配合动手学深度学习比较系统。</li>
</ul>
<ol>
<li><strong>英文 MOOC / 课程</strong></li>
</ol>
<ul>
<li><strong>Coursera</strong> 上 Andrew Ng 的“Deep Learning Specialization”有基础部分，但 Transformer 细节不多，可搭配其他资料。</li>
<li><strong>fast.ai NLP Course</strong> (部分资料已过时，但思路和实践方法仍具参考价值)。</li>
<li><strong>Stanford CS324 (Large Language Models)</strong>：最新一期课程专门聚焦 LLM，包括安全/对抗、社会影响等专题。</li>
</ul>
<ol>
<li><strong>社区 &amp; 开源项目</strong></li>
</ol>
<ul>
<li><strong>Hugging Face</strong>：最主要的 Transformers 库；社区中有众多模型、Datasets、Spaces（可在 Spaces 上试验 Demo）。</li>
<li><strong>OpenAI API 文档</strong>：如果想调用 GPT-3.5 / GPT-4 等，官方文档有许多最佳实践和安全准则说明。</li>
<li><strong>DeepSpeed / Megatron-LM</strong>：微软和 NVIDIA 提供的大规模分布式训练框架，大模型训练常用。</li>
</ul>
<hr>
<h3 id="时间与阶段安排简要示意"><a href="#时间与阶段安排简要示意" class="headerlink" title="时间与阶段安排简要示意"></a>时间与阶段安排简要示意</h3><ul>
<li><strong>第 1 阶段（1~2 周）</strong>：NLP 基础 + 注意力 &amp; seq2seq 回顾</li>
<li><strong>第 2 阶段（1~2 周）</strong>：阅读并理解 Transformer 论文；Hugging Face 实践</li>
<li><strong>第 3 阶段（2~3 周）</strong>：掌握主流 LLM（BERT、GPT 等），学习预训练、微调、RLHF</li>
<li><strong>第 4 阶段（2~3 周）</strong>：研究 LLM 安全与对抗性攻防；阅读安全论文和案例</li>
<li><strong>第 5 阶段（2~4 周）</strong>：动手实践安全攻击与防御，在开源模型上做实验，撰写研究报告</li>
</ul>
<p>整体可在<strong>3~4 个月左右</strong>形成一定深度的知识体系和实践经验。如果时间更紧，可以适度缩短各阶段时长，侧重于阅读关键论文和进行安全攻击/防御的实验研究。</p>
<hr>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>这条学习路径旨在<strong>让你从 CNN 的初阶深度学习经验</strong>，快速衔接到<strong>Transformer 的理论与实践</strong>，再进一步切入<strong>LLM 安全与对抗研究</strong>。</p>
<ul>
<li><strong>关键成功要素</strong>：一边看论文/教程，一边快速上手代码实验，多记录实验结果与心得，形成自己的理解。</li>
<li><strong>安全研究进阶</strong>：在掌握常见攻击与防御后，可以进阶到对抗性样本生成、模型检测与审计机制、隐私保护训练、AI 伦理与合规等更深层次的问题。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://kunlii.github.io/blog/2025/02/08/LLM%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/%E7%A7%91%E7%A0%94/" rel="tag">科研</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/blog/2025/03/24/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-GenAI%20Advertising%20Risks%20of%20Personalizing%20Ads%20with%20LLMs/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            论文记录-GenAI Advertising: Risks of Personalizing Ads with LLMs
          
        </div>
      </a>
    
    
      <a href="/blog/2025/02/01/2024%E5%B9%B4%E5%86%B3%E7%AE%97%E5%8F%8A2025%E5%B9%B4%E9%A2%84%E7%AE%97/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">2024年决算及2025年预算</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "VqvB5y0diPuBd9JFy4kQbHxz-gzGzoHsz",
    app_key: "Gmmb4mTXjSDf8ndcJ0ex4Ufi",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2026
        <i class="ri-heart-fill heart_icon"></i> Kun Li
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/blog/"><img src="/blog/images/ayer.png" alt="左边"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog/friends">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/blog/js/jquery-2.0.3.min.js"></script>


<script src="/blog/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/blog/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/blog/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/blog/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
<div id="waifu">
    <div id="waifu-message"></div>
    <div class="waifu-tool">
        <span class="icon-next"></span>
        <span class="icon-home"></span>
        <span class="icon-message"></span>
        <span class="icon-camera"></span>
        <span class="icon-volumeup"></span>
        <span class="icon-volumedown"></span>
        <span class="icon-about"></span>
        <span class="icon-cross"></span>
    </div>
    <canvas id="live2d2" style="top:150px"></canvas>
    <canvas id="live2d4" style="top:150px"></canvas>
</div>
<!--    src 中改为你存放的路径    -->
<script src="/blog/dist/live2d_bundle.js"></script>
<script async type="module" src="/blog/js/waifu-tips.js"></script>

</body>

</html>